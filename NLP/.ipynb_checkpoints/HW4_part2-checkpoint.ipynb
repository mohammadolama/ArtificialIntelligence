{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=center>\n",
    "\t\t\n",
    "<p></p>\n",
    "<p></p>\n",
    "<font size=5>\n",
    "In the Name of God\n",
    "<font/>\n",
    " <p></p>\n",
    " <br/>\n",
    "\t\t<div align=center>\n",
    "\t\t    <size=3.5>\n",
    "\t\t\t    <br />\n",
    "Practical Assignment 4- part 2\n",
    "              <font  size=4>\n",
    "            \t<br/>\n",
    "              N-grams\n",
    "\t\t\t</font>\n",
    "    <br/>\n",
    "    <br/>\n",
    " </div>\n",
    "<hr/>\n",
    "Artifical Intelligence - Dr. GholamReza GhassemSani\n",
    "</font>\n",
    "  <p></p>\n",
    " <br/>\n",
    "Sharif University of Technology\n",
    "<p></p>\n",
    "Spring 2022\n",
    "\n",
    "\n",
    "<br/>\n",
    "<br/>\n",
    "<font size=4 color=red>\n",
    " </font>\n",
    "                <br/>\n",
    "    </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Assignment Details\n",
    "**Due date: 21/4/1401 (at 11:59pm)**<br>\n",
    "You are free to collaborate but solutions must be written up individually.\n",
    "Collaborators **must** be acknowledged.<br>\n",
    "Submissions with more than 100 hours delay will not be graded.<br>Submissions with less than\n",
    "50 hours delay will be penalized by the following rule:<br>\n",
    "**Penalized mark = M * (100 – D) / 100** <br>\n",
    "Where M = the mark achieved from your solution and D is number of hours passed the\n",
    "deadline. Submissions within 50 < X ≤ 100 hours delay will be penalized by P.M. = M * 0.5.<br>\n",
    "Submit your answers on quera."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "student_number = 98100497\n",
    "\n",
    "Name = Mohammad Ali\n",
    "\n",
    "Last_Name = Olama"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "In this assignment you will be implementing N-gram language models. N-gram language models assume that each word $wi$\n",
    "depends only on the N−1 preceding words. you will be using these language models for:\n",
    "*   calculating the probability of sentences\n",
    "*   predicting next word of a query\n",
    "\n",
    "###  Dataset\n",
    "Download dataset from [here](https://drive.google.com/drive/folders/1OOjVMG61mrBmGDh2KgPrO0HqrMPj-M0j?usp=sharing).  \n",
    "The dataset you will be working with for this assignment are available as `train.pickle` and `test.pickle` files. Each file includes a list of lists and each list contains tokens of a sentences/sentences.  \n",
    " Run the codes below to know more about the data \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('train.pickle', 'rb') as handle:\n",
    "    train_data = pickle.load(handle)\n",
    "\n",
    "with open('test.pickle', 'rb') as handle:\n",
    "    test_data = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the cell below we have a sample tokenized sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['lol', 'my', 'fave', 'besides', 'prince', 'royce', 'of', 'course']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# task 1 (Preprocessing)\n",
    "At first explain what other preprocessing does the data need and and why then implement your suggested preprocessing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Explain your answers here*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Begin your code\n",
    "import re\n",
    "# i think it would be good to make every sentence in the corpus to be lowercase.\n",
    "# i think it's a good idea to remove any special character from the corpus.\n",
    "\n",
    "res =[]\n",
    "for t in train_data:\n",
    "    temp = []\n",
    "    for x in t:\n",
    "        x = x.lower()\n",
    "        y = re.sub(r\"[^a-zA-Z0-9?!. ]+\", \"\", x)\n",
    "        temp.append(y)\n",
    "        if y != x:\n",
    "            x = y\n",
    "    t = [x for x in temp if x !='']\n",
    "    res.append(t)\n",
    "train_data = res\n",
    "\n",
    "res1 =[]\n",
    "for t in test_data:\n",
    "    temp = []\n",
    "    for x in t:\n",
    "        x = x.lower()\n",
    "        y = re.sub(r\"[^a-zA-Z0-9?!. ]+\", \"\", x)\n",
    "        temp.append(y)\n",
    "        if y != x:\n",
    "            x = y\n",
    "    t = [x for x in temp if x !='']\n",
    "    res1.append(t)\n",
    "test_data = res1\n",
    "\n",
    "### End your code\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2 (Counting N-grams)\n",
    "Fill out the following code block to count the unigrams, bigrams and trigrams in the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def LanguageModel(data) -> dict:\n",
    "    ### you need to use your preprocessed data and count the unigram,bigram and trigrams \n",
    "    \n",
    "    ### Begin your code\n",
    "    unigram_counts = {}\n",
    "    bigram_counts = {}\n",
    "    trigram_counts = {}\n",
    "    \n",
    "    for t in train_data:\n",
    "        for x in t:\n",
    "            unigram_counts[x] = unigram_counts.get(x,0) + 1\n",
    "    \n",
    "    for t in train_data:\n",
    "        for i in range(len(t) - 2 + 1):\n",
    "            big = t[i : i + 2]\n",
    "            bigram = \" \".join(big)\n",
    "            bigram_counts[bigram] = bigram_counts.get(bigram,0) + 1\n",
    "    \n",
    "    for t in train_data:\n",
    "        for i in range(len(t) - 3 + 1):\n",
    "            tri = t[i : i + 3]\n",
    "            trigram = \" \".join(tri)\n",
    "            trigram_counts[trigram] = trigram_counts.get(trigram,0) + 1\n",
    "    ### End your code\n",
    "     \n",
    "    ### return dictinaries with n-grams as keys and counts of n-grams as values\n",
    "    return unigram_counts, bigram_counts, trigram_counts\n",
    "\n",
    "unigram_counts, bigram_counts, trigram_counts = LanguageModel(train_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3 (Laplace Smoothing)\n",
    "In natural language:\n",
    "*   A small number of events (e.g. words) occur with high frequency\n",
    "*   large number of events occur with very low frequency\\\n",
    "So we can’t actually evaluate our MLE models on\n",
    "unseen test data because both are likely to contain words/n-grams that these models assign zero probability to. In order to handle this problem we can use **1(laplace)smoothing** .  \n",
    "Fill out the following code cell to calculate the probability of unigram, bigram and trigram.\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do not forget to add 1(laplace)smoothing or othe methods of smoothing(which you need to explain in another markdown cell)\n",
    "def calc_unigram_p(w_1 , total_words , total_unique_words , unigram_counts): # p(w_1)\n",
    "    ### Begin your code\n",
    "    a = unigram_counts.get(w_1 , 0)\n",
    "    p = (a+1) / (len(total_words) + len(total_unique_words))\n",
    "\n",
    "    ### End your code\n",
    "    return p\n",
    "    \n",
    "def calc_bigram_p(w_1,w_2 , total_words , total_unique_words , unigram_counts , bigram_counts): # p(w_2|w_1)\n",
    "    ### Begin your code\n",
    "    l = [w_1 , w_2]\n",
    "    st = \" \".join(l)\n",
    "    a = bigram_counts.get(st , 0)\n",
    "    b = unigram_counts.get(w_1 , 0)\n",
    "    p = (a + 1) / (b + len(total_unique_words))\n",
    "    ### End your code\n",
    "    return p\n",
    "\n",
    "def calc_trigram_p(w_1,w_2,w_3 , total_words , total_unique_words , bigram_counts , trigram_counts): # p(w_3|w_2,w_1)\n",
    "    ### Begin your code\n",
    "    l1 = [w_1 , w_2 , w_3]\n",
    "    st1 = \" \".join(l1)\n",
    "    a = trigram_counts.get(st1 , 0)\n",
    "    l2 = [w_1 , w_2]\n",
    "    st2 = \" \".join(l2)\n",
    "    b = bigram_counts.get(st2 , 0)\n",
    "    \n",
    "    p = (a + 1) / (b + len(total_unique_words))\n",
    "    ### End your code\n",
    "    return p\n",
    "\n",
    "\n",
    "def count_words(data):\n",
    "    total_words = []\n",
    "    total_unique_words = []\n",
    "    \n",
    "    for t in train_data:\n",
    "        for x in t:\n",
    "            total_words.append(x)\n",
    "    \n",
    "    total_unique_words = list(set(total_words))\n",
    "    return total_words , total_unique_words\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 4 (Probability of sentences)\n",
    "At this part first you need to choose 2 sentence(with more than 4 words) from the test data and  calculate the probability of the chosen sentences under each language model(unigram,bigram and trigram) as expplained below:\n",
    "  \n",
    "We will treat each sentence as a sequence of terms $(w_1, \\ldots, w_n)$ whose probability under unigram model is computed as :\n",
    "$$\n",
    "P(w_1, \\ldots, w_n) = P(w_1)P(w_2)\\cdots P(w_n),\n",
    "$$\n",
    "under bigram model is computed as:\n",
    "$$\n",
    "P(w_1, \\ldots, w_n) = P(w_1)P(w_2\\mid w_1)\\cdots P(w_n\\mid w_{n-1}),\n",
    "$$\n",
    "and under trigram model is computed as:\n",
    "$$\n",
    "P(w_1, \\ldots, w_n) = P(w_1)P(w_2\\mid w_1)P(w_3\\mid w_1,w_2)\\cdots P(w_n\\mid w_{n-2},w_{n-1}),\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first sentence is:  seems difficult to find people on twitter . found nytimes bbcscitech but mainly hit and miss ... any suggestions for finding content sources\n",
      "under unigram model:  1.6239398077647473e-80\n",
      "under bigram model:  7.301167756714353e-107\n",
      "under trigram model:  2.8984937673175475e-124\n",
      "********************************************************************************\n",
      "second sentence is:  sweet merciful awesome . just do nt get cocky boys .\n",
      "under unigram model:  8.854997082591238e-35\n",
      "under bigram model:  1.0737357184517238e-49\n",
      "under trigram model:  6.744673716319555e-59\n"
     ]
    }
   ],
   "source": [
    "    ### Begin your code\n",
    "\n",
    "#     i choose first and second sentences from test_data.\n",
    "\n",
    "    def calc_uni(st1 , total_words , total_unique_words , unigram_counts):\n",
    "        p = 1.0\n",
    "        for i in range(len(st1)):\n",
    "            p = p * calc_unigram_p(st1[i] , total_words , total_unique_words , unigram_counts)\n",
    "        return p\n",
    "\n",
    "    def calc_bi(st1 , total_words , total_unique_words , unigram_counts , bigram_counts):\n",
    "        p = 1.0\n",
    "        for i in range(len(st1)):\n",
    "            if i == 0:\n",
    "                p = p * calc_unigram_p(st1[i] , total_words , total_unique_words , unigram_counts)\n",
    "            else:\n",
    "                p = p * calc_bigram_p(st1[i],st1[i-1] , total_words , total_unique_words , unigram_counts , bigram_counts)\n",
    "        return p\n",
    "    \n",
    "    def calc_tri(st1 , total_words , total_unique_words , unigram_counts , bigram_counts , trigram_counts):\n",
    "        p = 1.0\n",
    "        for i in range(len(st1)):\n",
    "            if i == 0:\n",
    "                p = p * calc_unigram_p(st1[i] , total_words , total_unique_words , unigram_counts)\n",
    "            elif i== 1:\n",
    "                p = p * calc_bigram_p(st1[i],st1[i-1] , total_words , total_unique_words , unigram_counts , bigram_counts)\n",
    "            else :\n",
    "                p = p * calc_trigram_p(st1[i],st1[i-1],st1[i-2] , total_words , total_unique_words , bigram_counts , trigram_counts)           \n",
    "        return p\n",
    "\n",
    "    \n",
    "    total_words , total_unique_words = count_words(train_data)\n",
    "    \n",
    "    st1 = test_data[0]\n",
    "    st2 = test_data[1]\n",
    "    print(\"first sentence is: \" , \" \".join(st1))\n",
    "    \n",
    "    \n",
    "    print(\"under unigram model: \" , str(calc_uni(st1 , total_words , total_unique_words , unigram_counts)))\n",
    "    print(\"under bigram model: \" ,str(calc_bi(st1 , total_words , total_unique_words , unigram_counts , bigram_counts)))\n",
    "    print(\"under trigram model: \" ,str(calc_tri(st1 , total_words , total_unique_words , unigram_counts , bigram_counts , trigram_counts)))\n",
    "    \n",
    "    print(80 * \"*\")\n",
    "    print(\"second sentence is: \" , \" \".join(st2))\n",
    "    \n",
    "    print(\"under unigram model: \" , str(calc_uni(st2 , total_words , total_unique_words , unigram_counts)))\n",
    "    print(\"under bigram model: \" ,str(calc_bi(st2 , total_words , total_unique_words , unigram_counts , bigram_counts)))\n",
    "    print(\"under trigram model: \" ,str(calc_tri(st2 , total_words , total_unique_words , unigram_counts , bigram_counts , trigram_counts)))\n",
    "    \n",
    "    ### End your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 5 (Finding The 3 Most Probable Next Words)\n",
    "In this part you are given a query including 2 words.  \n",
    "Query = i am  \n",
    "You need to find the 2 words that are more likely to be the Word that follows `am` under each language model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i am . .\n",
      "i am i m\n",
      "i am so excited\n",
      "********************************************************************************\n",
      "trigram part with more words =)\n",
      "i am so excited to see you there !\n"
     ]
    }
   ],
   "source": [
    "    ### Begin your code\n",
    "    \n",
    "#     unigram part\n",
    "\n",
    "    query = \"i am\"\n",
    "    for i in range(2):\n",
    "        prob = -1\n",
    "        word = \"\"\n",
    "        for t in total_unique_words:\n",
    "            a = calc_unigram_p(t , total_words , total_unique_words , unigram_counts)\n",
    "            if a > prob:\n",
    "                prob = a\n",
    "                word = t\n",
    "        query = query + \" \" + word\n",
    "    print(query)\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "#     bigram part\n",
    "\n",
    "    query = \"i am\"\n",
    "    for i in range(1 , 3):\n",
    "        prob = -1\n",
    "        word = \"\"\n",
    "        for t in total_unique_words:\n",
    "            a = calc_bigram_p(query.split(\" \")[i] , t , total_words , total_unique_words , unigram_counts , bigram_counts)\n",
    "            if a >= prob:\n",
    "                prob = a\n",
    "                word = t\n",
    "        query = query + \" \" + word\n",
    "    print(query)\n",
    "    \n",
    "    \n",
    "    \n",
    "#     trigram part\n",
    "    query = \"i am\"\n",
    "    for i in range(0 , 2):\n",
    "        prob = -1\n",
    "        word = \"\"\n",
    "        for t in total_unique_words:\n",
    "            a = calc_trigram_p(query.split(\" \")[i],query.split(\" \")[i+1],t , total_words , total_unique_words , bigram_counts , trigram_counts)\n",
    "            if a >= prob:\n",
    "                prob = a\n",
    "                word = t\n",
    "        query = query + \" \" + word\n",
    "    print(query)\n",
    "            \n",
    "        \n",
    "        #         trigram part with more words =)\n",
    "\n",
    "print(80 * \"*\")\n",
    "print(\"trigram part with more words =)\")\n",
    "        \n",
    "\n",
    "    query = \"i am\"\n",
    "    for i in range(0 , 7):\n",
    "        prob = -1\n",
    "        word = \"\"\n",
    "        for t in total_unique_words:\n",
    "            a = calc_trigram_p(query.split(\" \")[i],query.split(\" \")[i+1],t , total_words , total_unique_words , bigram_counts , trigram_counts)\n",
    "            if a >= prob:\n",
    "                prob = a\n",
    "                word = t\n",
    "        query = query + \" \" + word\n",
    "    print(query)\n",
    "\n",
    "\n",
    "    \n",
    "    ### End your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note that :\n",
    "***Your answers will be evaluated based on correct implementation of the above functions.**  \n",
    "***Each task has 20 points**\n",
    "# The End"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "3e4e885df77660ca7e0b087d607dd7176d5ad4338e5d26cc40a96e95894b80be"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
